"""
Google Maps ë¦¬ë·° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
2025.11.16 ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ í‘œì¤€í™”

ì£¼ìš” ê¸°ëŠ¥:
1. ì¤‘ë³µ ë¦¬ë·° ì œê±° (review_id ê¸°ì¤€)
2. ë¹ˆ í…ìŠ¤íŠ¸ ë˜ëŠ” ë„ˆë¬´ ì§§ì€ ë¦¬ë·° í•„í„°ë§ (20ì ë¯¸ë§Œ)
3. NULL ê°’ ì²˜ë¦¬
4. ë‚ ì§œ í˜•ì‹ í‘œì¤€í™” (2025.11.16 ê¸°ì¤€)
5. íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ì´ëª¨ì§€ëŠ” [EMOJI_name] í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
6. í´ë¦¬ë‹ (URL, HTML, ì œì–´ë¬¸ì ì œê±°. ì „í™”ë²ˆí˜¸Â·ì´ë©”ì¼ ë§ˆìŠ¤í‚¹. ë‹¤ì¤‘ ê³µë°± ì •ë¦¬)
"""

import json
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set
from collections import defaultdict
import unicodedata


class EmojiConverter:
    """ì´ëª¨ì§€ë¥¼ í…ìŠ¤íŠ¸ íƒœê·¸ë¡œ ë³€í™˜"""
    
    # ì£¼ìš” ì´ëª¨ì§€ ë§¤í•‘
    EMOJI_MAP = {
        'ğŸ˜€': 'grinning', 'ğŸ˜ƒ': 'smiley', 'ğŸ˜„': 'smile', 'ğŸ˜': 'grin',
        'ğŸ˜†': 'laughing', 'ğŸ˜…': 'sweat_smile', 'ğŸ¤£': 'rofl', 'ğŸ˜‚': 'joy',
        'ğŸ™‚': 'slightly_smiling', 'ğŸ™ƒ': 'upside_down', 'ğŸ˜‰': 'wink', 'ğŸ˜Š': 'blush',
        'ğŸ˜‡': 'innocent', 'ğŸ¥°': 'smiling_face_with_hearts', 'ğŸ˜': 'heart_eyes', 'ğŸ¤©': 'star_struck',
        'ğŸ˜˜': 'kissing_heart', 'ğŸ˜—': 'kissing', 'â˜ºï¸': 'relaxed', 'ğŸ˜š': 'kissing_closed_eyes',
        'ğŸ˜™': 'kissing_smiling_eyes', 'ğŸ¥²': 'smiling_face_with_tear', 'ğŸ˜‹': 'yum', 'ğŸ˜›': 'stuck_out_tongue',
        'ğŸ˜œ': 'stuck_out_tongue_winking_eye', 'ğŸ¤ª': 'zany_face', 'ğŸ˜': 'stuck_out_tongue_closed_eyes',
        'ğŸ¤‘': 'money_mouth', 'ğŸ¤—': 'hugs', 'ğŸ¤­': 'hand_over_mouth', 'ğŸ¤«': 'shushing',
        'ğŸ¤”': 'thinking', 'ğŸ¤': 'zipper_mouth', 'ğŸ¤¨': 'raised_eyebrow', 'ğŸ˜': 'neutral_face',
        'ğŸ˜‘': 'expressionless', 'ğŸ˜¶': 'no_mouth', 'ğŸ˜': 'smirk', 'ğŸ˜’': 'unamused',
        'ğŸ™„': 'rolling_eyes', 'ğŸ˜¬': 'grimacing', 'ğŸ¤¥': 'lying_face', 'ğŸ˜Œ': 'relieved',
        'ğŸ˜”': 'pensive', 'ğŸ˜ª': 'sleepy', 'ğŸ¤¤': 'drooling', 'ğŸ˜´': 'sleeping',
        'ğŸ˜·': 'mask', 'ğŸ¤’': 'face_with_thermometer', 'ğŸ¤•': 'face_with_head_bandage',
        'ğŸ¤¢': 'nauseated', 'ğŸ¤®': 'vomiting', 'ğŸ¤§': 'sneezing', 'ğŸ¥µ': 'hot',
        'ğŸ¥¶': 'cold', 'ğŸ¥´': 'woozy', 'ğŸ˜µ': 'dizzy', 'ğŸ¤¯': 'exploding_head',
        'ğŸ˜•': 'confused', 'ğŸ˜Ÿ': 'worried', 'ğŸ™': 'slightly_frowning', 'â˜¹ï¸': 'frowning',
        'ğŸ˜®': 'open_mouth', 'ğŸ˜¯': 'hushed', 'ğŸ˜²': 'astonished', 'ğŸ˜³': 'flushed',
        'ğŸ¥º': 'pleading', 'ğŸ˜¦': 'frowning_open_mouth', 'ğŸ˜§': 'anguished', 'ğŸ˜¨': 'fearful',
        'ğŸ˜°': 'cold_sweat', 'ğŸ˜¥': 'disappointed_relieved', 'ğŸ˜¢': 'cry', 'ğŸ˜­': 'sob',
        'ğŸ˜±': 'scream', 'ğŸ˜–': 'confounded', 'ğŸ˜£': 'persevere', 'ğŸ˜': 'disappointed',
        'ğŸ˜“': 'sweat', 'ğŸ˜©': 'weary', 'ğŸ˜«': 'tired', 'ğŸ¥±': 'yawning',
        'ğŸ˜¤': 'triumph', 'ğŸ˜¡': 'rage', 'ğŸ˜ ': 'angry', 'ğŸ¤¬': 'cursing',
        'ğŸ‘': 'thumbs_up', 'ğŸ‘': 'thumbs_down', 'ğŸ‘': 'clap', 'ğŸ™Œ': 'raised_hands',
        'ğŸ‘': 'open_hands', 'ğŸ¤²': 'palms_up', 'ğŸ¤': 'handshake', 'ğŸ™': 'pray',
        'âœ¨': 'sparkles', 'â­': 'star', 'ğŸŒŸ': 'glowing_star', 'ğŸ’«': 'dizzy_star',
        'âœ…': 'check', 'âŒ': 'x', 'â­•': 'o', 'â—': 'exclamation',
        'â“': 'question', 'ğŸ’¯': 'hundred', 'ğŸ”¥': 'fire', 'ğŸ’¥': 'boom',
        'â¤ï¸': 'heart', 'ğŸ§¡': 'orange_heart', 'ğŸ’›': 'yellow_heart', 'ğŸ’š': 'green_heart',
        'ğŸ’™': 'blue_heart', 'ğŸ’œ': 'purple_heart', 'ğŸ–¤': 'black_heart', 'ğŸ¤': 'white_heart',
        'ğŸ¤': 'brown_heart', 'ğŸ’”': 'broken_heart', 'â£ï¸': 'heart_exclamation', 'ğŸ’•': 'two_hearts',
        'ğŸ’': 'revolving_hearts', 'ğŸ’“': 'heartbeat', 'ğŸ’—': 'heartpulse', 'ğŸ’–': 'sparkling_heart',
        'ğŸ’˜': 'cupid', 'ğŸ’': 'gift_heart', 'ğŸ’Ÿ': 'heart_decoration',
        'ğŸ•': 'pizza', 'ğŸ”': 'hamburger', 'ğŸŸ': 'fries', 'ğŸŒ­': 'hotdog',
        'ğŸ¥ª': 'sandwich', 'ğŸŒ®': 'taco', 'ğŸŒ¯': 'burrito', 'ğŸ¥™': 'stuffed_flatbread',
        'ğŸ¥—': 'salad', 'ğŸ': 'spaghetti', 'ğŸœ': 'ramen', 'ğŸ²': 'stew',
        'ğŸ›': 'curry', 'ğŸ£': 'sushi', 'ğŸ±': 'bento', 'ğŸ¥Ÿ': 'dumpling',
        'ğŸ¤': 'fried_shrimp', 'ğŸ™': 'rice_ball', 'ğŸš': 'rice', 'ğŸ˜': 'rice_cracker',
        'ğŸ¥': 'fish_cake', 'ğŸ¥®': 'moon_cake', 'ğŸ¢': 'oden', 'ğŸ¡': 'dango',
        'ğŸ§': 'shaved_ice', 'ğŸ¨': 'ice_cream', 'ğŸ¦': 'soft_ice_cream', 'ğŸ¥§': 'pie',
        'ğŸ§': 'cupcake', 'ğŸ°': 'cake', 'ğŸ‚': 'birthday_cake', 'ğŸ®': 'custard',
        'ğŸ­': 'lollipop', 'ğŸ¬': 'candy', 'ğŸ«': 'chocolate', 'ğŸ¿': 'popcorn',
        'ğŸ©': 'donut', 'ğŸª': 'cookie', 'ğŸŒ°': 'chestnut', 'ğŸ¥œ': 'peanuts',
        'â˜•': 'coffee', 'ğŸµ': 'tea', 'ğŸ§ƒ': 'juice_box', 'ğŸ¥¤': 'cup_with_straw',
        'ğŸ§‹': 'bubble_tea', 'ğŸ¶': 'sake', 'ğŸº': 'beer', 'ğŸ»': 'beers',
        'ğŸ¥‚': 'champagne_glass', 'ğŸ·': 'wine', 'ğŸ¥ƒ': 'whisky', 'ğŸ¸': 'cocktail',
        'ğŸ¹': 'tropical_drink', 'ğŸ§‰': 'mate', 'ğŸ¾': 'champagne',
    }
    
    @classmethod
    def convert_emoji_to_tag(cls, text: str) -> str:
        """ì´ëª¨ì§€ë¥¼ [EMOJI_name] í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•œê¸€/ì˜ì–´/ìˆ«ìëŠ” ë³€í™˜í•˜ì§€ ì•ŠìŒ)"""
        result = text
        
        # 1ë‹¨ê³„: ë§¤í•‘ëœ ì´ëª¨ì§€ ë³€í™˜
        for emoji, name in cls.EMOJI_MAP.items():
            if emoji in result:
                result = result.replace(emoji, f'[EMOJI_{name}]')
        
        # 2ë‹¨ê³„: ë‚¨ì€ ì´ëª¨ì§€ë§Œ [EMOJI_unknown]ìœ¼ë¡œ ë³€í™˜
        # ê° ë¬¸ìë¥¼ ê²€ì‚¬í•˜ì—¬ ì‹¤ì œ ì´ëª¨ì§€ì¸ì§€ í™•ì¸
        def is_emoji(char):
            """ë¬¸ìê°€ ì´ëª¨ì§€ì¸ì§€ í™•ì¸ (ì˜ì–´, ìŠ¤í˜ì¸ì–´, í•œêµ­ì–´, ì´íƒˆë¦¬ì•„ì–´, í”„ë‘ìŠ¤ì–´ ë“± ì œì™¸)"""
            # ì´ë¯¸ ë³€í™˜ëœ íƒœê·¸ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ
            if char == '[' or char == ']':
                return False
            
            cp = ord(char)
            
            # ê¸°ë³¸ ASCII ë²”ìœ„ ì œì™¸ (ì˜ì–´, ìˆ«ì, ê¸°ë³¸ ê¸°í˜¸)
            if cp < 0x80:
                return False
            
            # ë¼í‹´ í™•ì¥ ë¬¸ì ì œì™¸ (ìŠ¤í˜ì¸ì–´, ì´íƒˆë¦¬ì•„ì–´, í”„ë‘ìŠ¤ì–´ ë“±)
            # Latin-1 Supplement: 0080-00FF (Ã¡, Ã©, Ã­, Ã³, Ãº, Ã±, Ã , Ã¨, Ã§ ë“±)
            if 0x0080 <= cp <= 0x00FF:
                return False
            
            # Latin Extended-A: 0100-017F (ì¶”ê°€ ë¼í‹´ ë¬¸ì)
            if 0x0100 <= cp <= 0x017F:
                return False
            
            # Latin Extended-B: 0180-024F (ì¶”ê°€ ë¼í‹´ ë¬¸ì)
            if 0x0180 <= cp <= 0x024F:
                return False
            
            # í•œê¸€ ë²”ìœ„ ì œì™¸ (AC00-D7AF: í•œê¸€ ìŒì ˆ, 1100-11FF: í•œê¸€ ìëª¨)
            if 0xAC00 <= cp <= 0xD7AF or 0x1100 <= cp <= 0x11FF:
                return False
            
            # ì‹¤ì œ ì´ëª¨ì§€ ë²”ìœ„ë§Œ True
            # ê°ì • ì´ëª¨í‹°ì½˜
            if 0x1F600 <= cp <= 0x1F64F:
                return True
            # ê¸°í˜¸ & í”½í† ê·¸ë¨
            if 0x1F300 <= cp <= 0x1F5FF:
                return True
            # êµí†µ & ì§€ë„
            if 0x1F680 <= cp <= 0x1F6FF:
                return True
            # êµ­ê¸°
            if 0x1F1E0 <= cp <= 0x1F1FF:
                return True
            # ì¶”ê°€ ì´ëª¨ì§€
            if 0x1F900 <= cp <= 0x1F9FF:
                return True
            # ìµœì‹  ì´ëª¨ì§€
            if 0x1FA70 <= cp <= 0x1FAFF:
                return True
            # ê¸°íƒ€ íŠ¹ìˆ˜ ì´ëª¨ì§€
            if 0x2600 <= cp <= 0x26FF:
                return True
            if 0x2700 <= cp <= 0x27BF:
                return True
            
            # ìœ ë‹ˆì½”ë“œ ì¹´í…Œê³ ë¦¬ë¡œ ì¶”ê°€ í™•ì¸ (Symbol, other)
            category = unicodedata.category(char)
            if category == 'So':
                # ë‹¨, CJK í†µí•© í•œì ë“±ì€ ì œì™¸
                if 0x4E00 <= cp <= 0x9FFF:  # CJK í†µí•© í•œì
                    return False
                if 0x3400 <= cp <= 0x4DBF:  # CJK í†µí•© í•œì í™•ì¥ A
                    return False
                return True
            
            return False
        
        # ë¬¸ìë³„ë¡œ ì²˜ë¦¬
        converted = []
        i = 0
        in_tag = False
        
        while i < len(result):
            char = result[i]
            
            # [EMOJI_xxx] íƒœê·¸ ë‚´ë¶€ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ
            if char == '[':
                # íƒœê·¸ ì‹œì‘ì¸ì§€ í™•ì¸
                if result[i:i+7] == '[EMOJI_':
                    in_tag = True
                    # íƒœê·¸ ëê¹Œì§€ ì°¾ê¸°
                    end = result.find(']', i)
                    if end != -1:
                        converted.append(result[i:end+1])
                        i = end + 1
                        in_tag = False
                        continue
            
            if not in_tag and is_emoji(char):
                converted.append('[EMOJI_unknown]')
            else:
                converted.append(char)
            
            i += 1
        
        return ''.join(converted)


class DateParser:
    """ë‚ ì§œ íŒŒì‹± ë° í‘œì¤€í™” (2025.11.16 ê¸°ì¤€)"""
    
    BASE_DATE = datetime(2025, 11, 16)
    
    @classmethod
    def parse_relative_date(cls, date_str: str) -> str:
        """
        ìƒëŒ€ì  ë‚ ì§œë¥¼ ì ˆëŒ€ ë‚ ì§œë¡œ ë³€í™˜
        ì˜ˆ: "16ì‹œê°„ ì „" -> "2025.11.15"
        """
        if not date_str or date_str.strip() == '':
            return ''
        
        # 'ìˆ˜ì •ì¼:' ì ‘ë‘ì‚¬ ì œê±°
        date_str = date_str.replace('ìˆ˜ì •ì¼:', '').strip()
        
        # í•œêµ­ì–´ íŒ¨í„´
        patterns = {
            r'(\d+)ì‹œê°„\s*ì „': lambda x: cls.BASE_DATE - timedelta(hours=int(x)),
            r'(\d+)ì¼\s*ì „': lambda x: cls.BASE_DATE - timedelta(days=int(x)),
            r'(\d+)ì£¼\s*ì „': lambda x: cls.BASE_DATE - timedelta(weeks=int(x)),
            r'(\d+)ë‹¬\s*ì „': lambda x: cls.BASE_DATE - timedelta(days=int(x)*30),
            r'(\d+)ê°œì›”\s*ì „': lambda x: cls.BASE_DATE - timedelta(days=int(x)*30),
            r'(\d+)ë…„\s*ì „': lambda x: cls.BASE_DATE - timedelta(days=int(x)*365),
        }
        
        for pattern, delta_func in patterns.items():
            match = re.search(pattern, date_str)
            if match:
                date = delta_func(match.group(1))
                return date.strftime('%Y.%m.%d')
        
        # ì´ë¯¸ í‘œì¤€ í˜•ì‹ì¸ ê²½ìš° ë°˜í™˜
        # YYYY.MM.DD, YYYY-MM-DD, YYYY/MM/DD ë“±
        date_formats = [
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
        ]
        
        for fmt in date_formats:
            match = re.search(fmt, date_str)
            if match:
                year, month, day = match.groups()
                return f"{year}.{month.zfill(2)}.{day.zfill(2)}"
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return date_str
    
    @classmethod
    def is_valid_date(cls, date_str: str) -> bool:
        """ë‚ ì§œ í˜•ì‹ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        if not date_str:
            return False
        
        pattern = r'\d{4}\.\d{2}\.\d{2}'
        return bool(re.match(pattern, date_str))


class TextCleaner:
    """í…ìŠ¤íŠ¸ ì •ì œ í´ë˜ìŠ¤"""
    
    @staticmethod
    def remove_urls(text: str) -> str:
        """URL ì œê±°"""
        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        return re.sub(url_pattern, '', text)
    
    @staticmethod
    def remove_html_tags(text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        return re.sub(r'<[^>]+>', '', text)
    
    @staticmethod
    def remove_control_characters(text: str) -> str:
        """ì œì–´ ë¬¸ì ì œê±° (íƒ­, ì¤„ë°”ê¿ˆ ì œì™¸)"""
        # ì œì–´ ë¬¸ì ì¤‘ \t, \n, \rë§Œ ìœ ì§€
        return ''.join(char for char in text if unicodedata.category(char)[0] != 'C' or char in '\t\n\r')
    
    @staticmethod
    def mask_phone_numbers(text: str) -> str:
        """ì „í™”ë²ˆí˜¸ ë§ˆìŠ¤í‚¹"""
        # í•œêµ­ ì „í™”ë²ˆí˜¸ íŒ¨í„´
        phone_patterns = [
            r'\b0\d{1,2}-\d{3,4}-\d{4}\b',  # 02-1234-5678, 010-1234-5678
            r'\b0\d{9,10}\b',  # 0212345678, 01012345678
        ]
        
        # ë¯¸êµ­ ì „í™”ë²ˆí˜¸ íŒ¨í„´
        us_phone_patterns = [
            r'\b\(\d{3}\)\s?\d{3}-\d{4}\b',  # (123) 456-7890
            r'\b\d{3}-\d{3}-\d{4}\b',  # 123-456-7890
        ]
        
        result = text
        for pattern in phone_patterns + us_phone_patterns:
            result = re.sub(pattern, '[PHONE]', result)
        
        return result
    
    @staticmethod
    def mask_emails(text: str) -> str:
        """ì´ë©”ì¼ ë§ˆìŠ¤í‚¹"""
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        return re.sub(email_pattern, '[EMAIL]', text)
    
    @staticmethod
    def normalize_whitespace(text: str) -> str:
        """ë‹¤ì¤‘ ê³µë°± ì •ë¦¬"""
        # íƒ­ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        text = text.replace('\t', ' ')
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r' +', ' ', text)
        
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ 2ê°œë¡œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
        lines = text.split('\n')
        lines = [line.strip() for line in lines]
        text = '\n'.join(lines)
        
        # ì „ì²´ í…ìŠ¤íŠ¸ì˜ ì•ë’¤ ê³µë°± ì œê±°
        return text.strip()
    
    @classmethod
    def clean_text(cls, text: str) -> str:
        """ì „ì²´ í´ë¦¬ë‹ í”„ë¡œì„¸ìŠ¤"""
        if not text:
            return ''
        
        # 1. URL ì œê±°
        text = cls.remove_urls(text)
        
        # 2. HTML íƒœê·¸ ì œê±°
        text = cls.remove_html_tags(text)
        
        # 3. ì œì–´ ë¬¸ì ì œê±°
        text = cls.remove_control_characters(text)
        
        # 4. ì „í™”ë²ˆí˜¸ ë§ˆìŠ¤í‚¹
        text = cls.mask_phone_numbers(text)
        
        # 5. ì´ë©”ì¼ ë§ˆìŠ¤í‚¹
        text = cls.mask_emails(text)
        
        # 6. ê³µë°± ì •ë¦¬
        text = cls.normalize_whitespace(text)
        
        return text


class ReviewPreprocessor:
    """ë¦¬ë·° ì „ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, min_text_length: int = 20):
        self.min_text_length = min_text_length
        self.seen_review_ids: Set[str] = set()
        self.stats = defaultdict(int)
    
    def is_valid_review(self, review: Dict) -> tuple[bool, str]:
        """
        ë¦¬ë·° ìœ íš¨ì„± ê²€ì¦
        Returns: (is_valid, reason)
        """
        # 1. review_id ì¤‘ë³µ ì²´í¬
        review_id = review.get('review_id', '')
        if not review_id:
            return False, 'no_review_id'
        
        if review_id in self.seen_review_ids:
            return False, 'duplicate'
        
        # 2. í…ìŠ¤íŠ¸ ì¡´ì¬ ë° ê¸¸ì´ ì²´í¬
        text = review.get('text', '')
        if text is None or text.strip() == '':
            return False, 'empty_text'
        
        if len(text.strip()) < self.min_text_length:
            return False, 'too_short'
        
        return True, 'valid'
    
    def handle_null_values(self, review: Dict) -> Dict:
        """NULL ê°’ ì²˜ë¦¬"""
        # ê¸°ë³¸ê°’ ì„¤ì •
        defaults = {
            'date': '',
            'language': 'unknown',
            'rating': 0,
            'review_id': '',
            'text': ''
        }
        
        processed = {}
        for key, default_value in defaults.items():
            value = review.get(key)
            if value is None or (isinstance(value, str) and value.strip() == ''):
                processed[key] = default_value
            else:
                processed[key] = value
        
        # ì¶”ê°€ í•„ë“œë„ í¬í•¨
        for key, value in review.items():
            if key not in processed:
                processed[key] = value if value is not None else ''
        
        return processed
    
    def preprocess_review(self, review: Dict, restaurant_info: Dict) -> Optional[Dict]:
        """ê°œë³„ ë¦¬ë·° ì „ì²˜ë¦¬"""
        # 1. NULL ê°’ ì²˜ë¦¬
        review = self.handle_null_values(review)
        
        # 2. ìœ íš¨ì„± ê²€ì¦
        is_valid, reason = self.is_valid_review(review)
        if not is_valid:
            self.stats[f'filtered_{reason}'] += 1
            return None
        
        # review_id ì¶”ê°€
        self.seen_review_ids.add(review['review_id'])
        
        # 3. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        original_text = review['text']
        
        # 3-1. ì´ëª¨ì§€ ë³€í™˜
        text_with_emoji_tags = EmojiConverter.convert_emoji_to_tag(original_text)
        
        # 3-2. í…ìŠ¤íŠ¸ í´ë¦¬ë‹
        cleaned_text = TextCleaner.clean_text(text_with_emoji_tags)
        
        # í´ë¦¬ë‹ í›„ ê¸¸ì´ ì¬í™•ì¸
        if len(cleaned_text.strip()) < self.min_text_length:
            self.stats['filtered_too_short_after_cleaning'] += 1
            return None
        
        # 4. ë‚ ì§œ í‘œì¤€í™”
        parsed_date = DateParser.parse_relative_date(review.get('date', ''))
        
        # 5. ì „ì²˜ë¦¬ëœ ë¦¬ë·° ìƒì„±
        processed_review = {
            'review_id': review['review_id'],
            'original_text': original_text,
            'cleaned_text': cleaned_text,
            'date': parsed_date,
            'date_valid': DateParser.is_valid_date(parsed_date),
            'language': review.get('language', 'unknown'),
            'rating': review.get('rating', 0),
            
            # ë ˆìŠ¤í† ë‘ ì •ë³´
            'restaurant_name': restaurant_info.get('name', ''),
            'restaurant_place_id': restaurant_info.get('place_id', ''),
            'restaurant_grid': restaurant_info.get('grid', ''),
            'restaurant_address': restaurant_info.get('address', ''),
            'restaurant_rating': restaurant_info.get('rating', 0),
            'restaurant_phone': restaurant_info.get('phone_number', ''),
            
            # ë©”íƒ€ ì •ë³´
            'char_count': len(cleaned_text),
            'word_count': len(cleaned_text.split()),
        }
        
        self.stats['processed'] += 1
        return processed_review
    
    def process_restaurant_file(self, file_path: Path) -> List[Dict]:
        """ë ˆìŠ¤í† ë‘ íŒŒì¼ ì²˜ë¦¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            restaurant_info = {
                'name': data.get('name'),
                'place_id': data.get('place_id'),
                'grid': data.get('grid'),
                'address': data.get('address'),
                'rating': data.get('rating'),
                'user_ratings_total': data.get('user_ratings_total'),
                'phone_number': data.get('phone_number'),
            }
            
            processed_reviews = []
            
            for review in data.get('reviews', []):
                processed = self.preprocess_review(review, restaurant_info)
                if processed:
                    processed_reviews.append(processed)
            
            return processed_reviews
            
        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            self.stats['errors'] += 1
            return []
    
    def process_all_files(self, input_dir: Path, output_dir: Path):
        """ëª¨ë“  íŒŒì¼ ì²˜ë¦¬"""
        input_dir = Path(input_dir)
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        all_processed_reviews = []
        
        # ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸°
        json_files = list(input_dir.rglob('*.json'))
        total_files = len(json_files)
        
        print(f"ì´ {total_files}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
        print(f"ê¸°ì¤€ ë‚ ì§œ: {DateParser.BASE_DATE.strftime('%Y.%m.%d')}")
        print(f"ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´: {self.min_text_length}ì\n")
        
        for idx, file_path in enumerate(json_files, 1):
            if idx % 100 == 0:
                print(f"ì§„í–‰ ì¤‘: {idx}/{total_files} ({idx/total_files*100:.1f}%)")
            
            processed_reviews = self.process_restaurant_file(file_path)
            all_processed_reviews.extend(processed_reviews)
        
        # ê²°ê³¼ ì €ì¥
        output_file = output_dir / 'preprocessed_reviews.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_processed_reviews, f, ensure_ascii=False, indent=2)
        
        # í†µê³„ ì €ì¥
        stats_output = {
            'total_files_processed': total_files,
            'total_reviews_processed': self.stats['processed'],
            'filtered_no_review_id': self.stats['filtered_no_review_id'],
            'filtered_duplicate': self.stats['filtered_duplicate'],
            'filtered_empty_text': self.stats['filtered_empty_text'],
            'filtered_too_short': self.stats['filtered_too_short'],
            'filtered_too_short_after_cleaning': self.stats['filtered_too_short_after_cleaning'],
            'errors': self.stats['errors'],
            'base_date': DateParser.BASE_DATE.strftime('%Y.%m.%d'),
            'min_text_length': self.min_text_length,
        }
        
        stats_file = output_dir / 'preprocessing_stats.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats_output, f, ensure_ascii=False, indent=2)
        
        # ìš”ì•½ ì¶œë ¥
        print("\n" + "="*60)
        print("ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("="*60)
        print(f"âœ… ì²˜ë¦¬ëœ ë¦¬ë·°: {self.stats['processed']:,}ê°œ")
        print(f"âŒ í•„í„°ë§ëœ ë¦¬ë·°:")
        print(f"   - review_id ì—†ìŒ: {self.stats['filtered_no_review_id']:,}ê°œ")
        print(f"   - ì¤‘ë³µ: {self.stats['filtered_duplicate']:,}ê°œ")
        print(f"   - ë¹ˆ í…ìŠ¤íŠ¸: {self.stats['filtered_empty_text']:,}ê°œ")
        print(f"   - ë„ˆë¬´ ì§§ìŒ (í´ë¦¬ë‹ ì „): {self.stats['filtered_too_short']:,}ê°œ")
        print(f"   - ë„ˆë¬´ ì§§ìŒ (í´ë¦¬ë‹ í›„): {self.stats['filtered_too_short_after_cleaning']:,}ê°œ")
        print(f"âš ï¸  ì—ëŸ¬: {self.stats['errors']:,}ê°œ")
        print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼:")
        print(f"   - {output_file}")
        print(f"   - {stats_file}")
        print("="*60)
        
        return all_processed_reviews


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²½ë¡œ ì„¤ì •
    input_dir = Path(r"E:\gitrepo\reivew-embedding\reviews")
    output_dir = Path(r"E:\gitrepo\reivew-embedding\preprocessed")
    
    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = ReviewPreprocessor(
        min_text_length=20  # 20ì ë¯¸ë§Œ í•„í„°ë§
    )
    
    # ì „ì²˜ë¦¬ ì‹¤í–‰
    reviews = preprocessor.process_all_files(
        input_dir=input_dir,
        output_dir=output_dir
    )
    
    print(f"\nì´ {len(reviews):,}ê°œì˜ ë¦¬ë·°ê°€ ì „ì²˜ë¦¬ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
