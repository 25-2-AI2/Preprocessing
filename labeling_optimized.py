import os
import sys
import json
import math
import asyncio
import argparse
import pandas as pd
from pathlib import Path
from datetime import datetime
from openai import AsyncOpenAI
from tqdm.asyncio import tqdm_asyncio
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
from openai import RateLimitError, APITimeoutError, APIConnectionError

# ============================================================
# ì„¤ì •
# ============================================================

# API í´ë¼ì´ì–¸íŠ¸
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ê¸°ë³¸ ê²½ë¡œ
DEFAULT_INPUT_DIR = "parquet_data"
DEFAULT_OUTPUT_DIR = "parquet_data"
CHECKPOINT_DIR = "label_data"

# í…ìŠ¤íŠ¸ ì»¬ëŸ¼ (DATA_SPECIFICATION.md ê¸°ì¤€)
TEXT_COL = "cleaned_text"
ID_COL = "review_id"

# ì„±ëŠ¥ ì„¤ì •
BATCH_SIZE = 50           # í•œ ë²ˆì— ë³´ë‚¼ ë¦¬ë·° ê°œìˆ˜
MAX_CONCURRENCY = 10      # ë™ì‹œ ì‹¤í–‰ ë°°ì¹˜ ìˆ˜
CHECKPOINT_INTERVAL = 5   # Nê°œ ë°°ì¹˜ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
MODEL = "gpt-4o-mini"     # ì†ë„ ìš°ì„  ëª¨ë¸

# ============================================================
# ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ (ì••ì¶• ë²„ì „)
# ============================================================

SYSTEM_PROMPT = """Aspect-based sentiment annotator for restaurant reviews.

For each review, assign scores (-2 to +2) and flags (0/1):
- food_score: taste, quality, drinks
- service_score: staff attitude, response speed  
- ambience_score: atmosphere, noise, interior
- price_score: value for money
- hygiene_score: cleanliness
- waiting_score: wait time for seating
- accessibility_score: location, parking
- racism_flag: discrimination (1 if present)
- cash_only_flag: cash-only payment (1 if mentioned)
- comment: 1-2 sentence Korean summary

Scoring guide:
+2: very positive (best, amazing, perfect)
+1: positive (good, nice)
0: neutral or not mentioned
-1: negative (disappointing, problematic)
-2: very negative (worst, disgusting, dangerous)

Rules:
- Score 0 if aspect not mentioned
- If mixed sentiment, use stronger absolute value
- Output ONLY a JSON array, same order as input
- Each object: id, all scores, both flags, comment"""

USER_TEMPLATE = """Reviews JSON:
{reviews_json}

Return JSON array with keys: id, food_score, service_score, ambience_score, price_score, hygiene_score, waiting_score, accessibility_score, racism_flag, cash_only_flag, comment"""

# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================

def get_paths(input_file: str):
    """ì…ë ¥ íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì¶œë ¥ ê²½ë¡œ ìƒì„±"""
    input_path = Path(input_file)
    stem = input_path.stem  # e.g., "reviews_part1"
    
    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    output_parquet = input_path.parent / f"{stem}_labeled.parquet"
    output_csv = Path(CHECKPOINT_DIR) / f"{stem}_labeled.csv"
    checkpoint_file = Path(CHECKPOINT_DIR) / f"checkpoint_{stem}.json"
    intermediate_file = Path(CHECKPOINT_DIR) / f"intermediate_{stem}.parquet"
    
    return {
        'input': input_path,
        'output_parquet': output_parquet,
        'output_csv': output_csv,
        'checkpoint': checkpoint_file,
        'intermediate': intermediate_file,
    }


def load_checkpoint(checkpoint_path: Path):
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    if checkpoint_path.exists():
        try:
            with open(checkpoint_path, 'r', encoding='utf-8') as f:
                checkpoint = json.load(f)
                print(f"âœ“ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: ë°°ì¹˜ {checkpoint['last_batch']+1}ë¶€í„° ì¬ì‹œì‘")
                return checkpoint
        except Exception as e:
            print(f"âš  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return None


def save_checkpoint(checkpoint_path: Path, last_batch: int, all_labels: list, completed_batches: set):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
    checkpoint = {
        'last_batch': last_batch,
        'labels': all_labels,
        'completed_batches': list(completed_batches),
        'timestamp': datetime.now().isoformat()
    }
    with open(checkpoint_path, 'w', encoding='utf-8') as f:
        json.dump(checkpoint, f, ensure_ascii=False)


def save_intermediate(intermediate_path: Path, df_original: pd.DataFrame, all_labels: list):
    """ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (Parquet)"""
    intermediate_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Noneì´ ì•„ë‹Œ ë¼ë²¨ë§Œ ìˆëŠ” í–‰ í™•ì¸
    valid_indices = [i for i, label in enumerate(all_labels) if label is not None]
    
    if not valid_indices:
        return
    
    # ìœ íš¨í•œ ë¼ë²¨ë¡œ DataFrame ìƒì„±
    labels_df = pd.DataFrame([all_labels[i] if all_labels[i] else {} for i in range(len(all_labels))])
    df_partial = pd.concat([df_original.reset_index(drop=True), labels_df.reset_index(drop=True)], axis=1)
    
    df_partial.to_parquet(intermediate_path, index=False)
    print(f"  ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {len(valid_indices)}/{len(all_labels)} ë¦¬ë·° ì²˜ë¦¬ë¨")


def cleanup_checkpoint(checkpoint_path: Path):
    """ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ"""
    if checkpoint_path.exists():
        checkpoint_path.unlink()
        print("âœ“ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")


# ============================================================
# API í˜¸ì¶œ í•¨ìˆ˜ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
# ============================================================

@retry(
    retry=retry_if_exception_type((RateLimitError, APITimeoutError, APIConnectionError)),
    wait=wait_exponential(multiplier=1, min=2, max=60),
    stop=stop_after_attempt(5),
    before_sleep=lambda retry_state: print(f"  â³ ì¬ì‹œë„ ëŒ€ê¸° ì¤‘... (ì‹œë„ {retry_state.attempt_number}/5)")
)
async def call_api(messages: list, sem: asyncio.Semaphore):
    """API í˜¸ì¶œ (Rate Limit ëŒ€ì‘)"""
    async with sem:
        resp = await client.chat.completions.create(
            model=MODEL,
            temperature=0,
            messages=messages,
            timeout=120
        )
        return resp


async def label_batch_async(
    texts: list,
    batch_idx: int,
    sem: asyncio.Semaphore,
) -> list:
    """ë°°ì¹˜ ë¼ë²¨ë§"""
    reviews_payload = [{"id": i, "text": str(t)} for i, t in enumerate(texts)]
    user_msg = USER_TEMPLATE.format(
        reviews_json=json.dumps(reviews_payload, ensure_ascii=False)
    )
    
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_msg},
    ]
    
    try:
        resp = await call_api(messages, sem)
        content = resp.choices[0].message.content.strip()
        
        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì œê±°)
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        content = content.strip()
        
        data_list = json.loads(content)
        
        if not isinstance(data_list, list) or len(data_list) != len(texts):
            raise ValueError(f"Output mismatch: expected {len(texts)}, got {len(data_list) if isinstance(data_list, list) else 'non-list'}")
        
        # id ê¸°ì¤€ ì •ë ¬ í›„ id ì œê±°
        data_list = sorted(data_list, key=lambda d: d.get("id", 0))
        for d in data_list:
            d.pop("id", None)
        
        return data_list
    
    except json.JSONDecodeError as e:
        print(f"  âŒ [ë°°ì¹˜ {batch_idx}] JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return [None] * len(texts)
    except Exception as e:
        print(f"  âŒ [ë°°ì¹˜ {batch_idx}] ì˜¤ë¥˜: {e}")
        return [None] * len(texts)


# ============================================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================================

async def main_async(input_file: str):
    print("=" * 60)
    print("ğŸš€ ë¦¬ë·° ë¼ë²¨ë§ ì‹œì‘")
    print("=" * 60)
    
    # ê²½ë¡œ ì„¤ì •
    paths = get_paths(input_file)
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not paths['input'].exists():
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {paths['input']}")
        sys.exit(1)
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_parquet(paths['input'])
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    if TEXT_COL not in df.columns:
        print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ '{TEXT_COL}'ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
        sys.exit(1)
    
    texts = df[TEXT_COL].astype(str).tolist()
    n = len(texts)
    num_batches = math.ceil(n / BATCH_SIZE)
    
    print(f"ğŸ“ ì…ë ¥: {paths['input']}")
    print(f"ğŸ“ ì¶œë ¥: {paths['output_parquet']}")
    print(f"ğŸ“Š ì´ ë¦¬ë·°: {n:,}ê°œ")
    print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}, ì´ ë°°ì¹˜: {num_batches}")
    print(f"âš¡ ë™ì‹œ ì‹¤í–‰: {MAX_CONCURRENCY}")
    print(f"ğŸ¤– ëª¨ë¸: {MODEL}")
    print("-" * 60)
    
    # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    checkpoint = load_checkpoint(paths['checkpoint'])
    if checkpoint:
        all_labels = checkpoint['labels']
        completed_batches = set(checkpoint.get('completed_batches', []))
        pending_batches = [b for b in range(num_batches) if b not in completed_batches]
    else:
        all_labels = [None] * n
        completed_batches = set()
        pending_batches = list(range(num_batches))
    
    print(f"ğŸ“‹ ì²˜ë¦¬ ëŒ€ê¸° ë°°ì¹˜: {len(pending_batches)}/{num_batches}")
    
    # ì„¸ë§ˆí¬ì–´ (ë™ì‹œì„± ì œí•œ)
    sem = asyncio.Semaphore(MAX_CONCURRENCY)
    
    # ì§„í–‰ ìƒí™© ì¶”ì 
    processed_count = 0
    lock = asyncio.Lock()
    
    async def handle_batch(batch_idx: int):
        nonlocal processed_count
        
        start = batch_idx * BATCH_SIZE
        end = min((batch_idx + 1) * BATCH_SIZE, n)
        batch_texts = texts[start:end]
        
        labels = await label_batch_async(batch_texts, batch_idx, sem)
        
        async with lock:
            all_labels[start:end] = labels
            completed_batches.add(batch_idx)
            processed_count += 1
            
            # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if processed_count % CHECKPOINT_INTERVAL == 0:
                save_checkpoint(paths['checkpoint'], batch_idx, all_labels, completed_batches)
                save_intermediate(paths['intermediate'], df, all_labels)
        
        return batch_idx
    
    # ë°°ì¹˜ ì²˜ë¦¬ (ì§„í–‰ë¥  í‘œì‹œ)
    tasks = [handle_batch(b) for b in pending_batches]
    
    try:
        await tqdm_asyncio.gather(
            *tasks,
            desc="ë¼ë²¨ë§ ì§„í–‰",
            total=len(pending_batches),
            unit="batch"
        )
    except KeyboardInterrupt:
        print("\nâš  ì¤‘ë‹¨ë¨! í˜„ì¬ê¹Œì§€ ê²°ê³¼ ì €ì¥ ì¤‘...")
        save_checkpoint(paths['checkpoint'], max(completed_batches) if completed_batches else 0, all_labels, completed_batches)
        save_intermediate(paths['intermediate'], df, all_labels)
        print("âœ“ ì €ì¥ ì™„ë£Œ. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
        return
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("í˜„ì¬ê¹Œì§€ ê²°ê³¼ ì €ì¥ ì¤‘...")
        save_checkpoint(paths['checkpoint'], max(completed_batches) if completed_batches else 0, all_labels, completed_batches)
        save_intermediate(paths['intermediate'], df, all_labels)
        print("âœ“ ì €ì¥ ì™„ë£Œ. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
        raise
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    print("-" * 60)
    print("ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    labels_df = pd.DataFrame(all_labels)
    df_labeled = pd.concat([df.reset_index(drop=True), labels_df.reset_index(drop=True)], axis=1)
    
    # Parquet ì €ì¥
    paths['output_parquet'].parent.mkdir(parents=True, exist_ok=True)
    df_labeled.to_parquet(paths['output_parquet'], index=False)
    
    # CSV ì €ì¥
    paths['output_csv'].parent.mkdir(parents=True, exist_ok=True)
    df_labeled.to_csv(paths['output_csv'], index=False, encoding="utf-8-sig")
    
    # ì‹¤íŒ¨í•œ í•­ëª© í™•ì¸
    failed_count = sum(1 for label in all_labels if label is None)
    success_count = n - failed_count
    
    print("=" * 60)
    print("âœ… ì™„ë£Œ!")
    print(f"   - ì„±ê³µ: {success_count:,}/{n:,} ({success_count/n*100:.1f}%)")
    if failed_count > 0:
        print(f"   - ì‹¤íŒ¨: {failed_count:,}ê°œ")
    print(f"   - Parquet: {paths['output_parquet']}")
    print(f"   - CSV: {paths['output_csv']}")
    print("=" * 60)
    
    # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
    cleanup_checkpoint(paths['checkpoint'])
    
    # ì¤‘ê°„ íŒŒì¼ ì •ë¦¬
    if paths['intermediate'].exists():
        paths['intermediate'].unlink()


# ============================================================
# ì—¬ëŸ¬ íŒŒì¼ ì¼ê´„ ì²˜ë¦¬
# ============================================================

async def process_multiple_files(input_files: list):
    """ì—¬ëŸ¬ íŒŒì¼ ìˆœì°¨ ì²˜ë¦¬"""
    for i, input_file in enumerate(input_files, 1):
        print(f"\n{'#' * 60}")
        print(f"# íŒŒì¼ {i}/{len(input_files)}: {input_file}")
        print(f"{'#' * 60}\n")
        await main_async(input_file)


# ============================================================
# CLI ì¸í„°í˜ì´ìŠ¤
# ============================================================

def parse_args():
    parser = argparse.ArgumentParser(
        description="Restaurant Review Labeling Tool",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
  python labeling_optimized.py parquet_data/reviews_part1.parquet
  
  # ì—¬ëŸ¬ íŒŒì¼ ì²˜ë¦¬
  python labeling_optimized.py parquet_data/reviews_part1.parquet parquet_data/reviews_part2.parquet
  
  # ì™€ì¼ë“œì¹´ë“œ ì‚¬ìš© (shellì—ì„œ)
  python labeling_optimized.py parquet_data/reviews_part*.parquet
  
  # ì„¤ì • ë³€ê²½
  python labeling_optimized.py input.parquet --batch-size 30 --concurrency 5
        """
    )
    
    parser.add_argument(
        'input_files',
        nargs='+',
        help='ì…ë ¥ parquet íŒŒì¼ ê²½ë¡œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)'
    )
    
    parser.add_argument(
        '--batch-size', '-b',
        type=int,
        default=BATCH_SIZE,
        help=f'ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: {BATCH_SIZE})'
    )
    
    parser.add_argument(
        '--concurrency', '-c',
        type=int,
        default=MAX_CONCURRENCY,
        help=f'ë™ì‹œ ì‹¤í–‰ ìˆ˜ (ê¸°ë³¸ê°’: {MAX_CONCURRENCY})'
    )
    
    parser.add_argument(
        '--model', '-m',
        type=str,
        default=MODEL,
        help=f'ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: {MODEL})'
    )
    
    parser.add_argument(
        '--text-column', '-t',
        type=str,
        default=TEXT_COL,
        help=f'í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: {TEXT_COL})'
    )
    
    return parser.parse_args()


# ============================================================
# ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# ============================================================

if __name__ == "__main__":
    args = parse_args()
    
    # ì „ì—­ ì„¤ì • ì—…ë°ì´íŠ¸
    BATCH_SIZE = args.batch_size
    MAX_CONCURRENCY = args.concurrency
    MODEL = args.model
    TEXT_COL = args.text_column
    
    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì • ë°©ë²•:")
        print("   - Windows CMD: set OPENAI_API_KEY=sk-your-key")
        print("   - PowerShell: $env:OPENAI_API_KEY=\"sk-your-key\"")
        sys.exit(1)
    
    # ì‹¤í–‰
    if len(args.input_files) == 1:
        asyncio.run(main_async(args.input_files[0]))
    else:
        asyncio.run(process_multiple_files(args.input_files))
